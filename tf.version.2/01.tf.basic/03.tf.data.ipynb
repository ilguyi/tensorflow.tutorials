{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `tf.data` API\n",
    "\n",
    "* References\n",
    "  * [`tf.data.Dataset` API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)\n",
    "  * [Importing data](https://www.tensorflow.org/guide/datasets) (Guideline Documents)\n",
    "  * [Data Input Pipeline Performance](https://www.tensorflow.org/guide/performance/datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.data` API has two (or three) new abstractions\n",
    "* `tf.data.Dataset` represents a sequence of elements, in which each element contains one or more Tensor objects. For example, in an image pipeline, the image data and a label\n",
    "  * Creating a source (e.g. Dataset.from_tensor_slices()) constructs a dataset from one or more `tf.Tensor` objects.\n",
    "    * `tf.data.Dataset.from_tensors()`\n",
    "    * `tf.data.Dataset.from_tensor_slices()`\n",
    "    * `tf.data.TFRecordDataset`:  TFRecord format을 읽을 때\n",
    "  * Applying a transformation (e.g. Dataset.batch()) constructs a dataset from one or more `tf.data.Dataset` objects.\n",
    "* `tf.data.Iterator` provides the main way to extract elements from a dataset. The operation returned by `Iterator.get_next()` yields the next element of a `Dataset` when executed, and typically acts as the interface between input pipeline code and your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MNIST dataset from `tf.keras.datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), (test_data, test_labels) = \\\n",
    "    tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 10\n",
    "print(\"label = {}\".format(train_labels[index]))\n",
    "plt.imshow(train_data[index].reshape(28, 28))\n",
    "plt.colorbar()\n",
    "#plt.gca().grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set N=50 for small dataset loading\n",
    "N = 50\n",
    "train_data = train_data[:N]\n",
    "train_labels = train_labels[:N]\n",
    "train_data = train_data / 255.\n",
    "train_data = train_data.astype(np.float32)\n",
    "train_labels = train_labels.astype(np.int32)\n",
    "\n",
    "test_data = test_data[:N]\n",
    "test_labels = test_labels[:N]\n",
    "test_data = test_data / 255.\n",
    "test_data = test_data.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input pipeline\n",
    "\n",
    "1. You must define a source. `tf.data.Dataset`.\n",
    "  * To construct a Dataset from some tensors in memory, you can use `tf.data.Dataset.from_tensors()` or `tf.data.Dataset.from_tensor_slices()`.\n",
    "  * Other methods\n",
    "    * `tf.data.TextLineDataset(filenames)`\n",
    "    * `tf.data.FixedLengthRecordDataset(filenames)`\n",
    "    * `tf.data.TFRecordDataset(filenames)`\n",
    "2. Transformation\n",
    "  * `Dataset.map()`: to apply a function to each element\n",
    "  * `Dataset.batch()`\n",
    "  * `Dataset.shuffle()`\n",
    "3. `Iterator`\n",
    "  * `Iterator.initializer`: which enables you to (re)initialize the iterator's state\n",
    "  * `Iterator.get_next()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Store data in `tf.data.Dataset`\n",
    "\n",
    "* `tf.data.Dataset.from_tensor_slices((features, labels))`\n",
    "* `tf.data.Dataset.from_generator(gen, output_types, output_shapes)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "print(train_dataset)\n",
    "print(train_dataset.output_shapes)\n",
    "print(train_dataset.output_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transformaion\n",
    "\n",
    "* `apply(transformation_func)`\n",
    "* `batch(batch_size)`\n",
    "* `concatenate(dataset)`\n",
    "* `map(map_func)`\n",
    "* `repeat(count=None)`\n",
    "  * count=max_epochs\n",
    "* `shuffle(buffer_size, seed=None, reshuffle_each_iteration=None)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_label(image, label):\n",
    "  label = tf.cast(label, dtype=tf.float32)\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.repeat(count=2)\n",
    "train_dataset = train_dataset.map(cast_label)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Iterator\n",
    "\n",
    "* In eager execution, we can directly sample the batch dataset from `tf.data.Dataset` instead of using `Iterator`.\n",
    "* We do not need to explicitly create an `tf.data.Iterator` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (x, y) in enumerate(train_dataset.take(10)):\n",
    "  print(\"step: {}  label: {}\".format(step, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General method for importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 2\n",
    "for epoch in range(max_epochs):\n",
    "  for step, (x, y) in enumerate(train_dataset):\n",
    "    print(\"step: {}  label: {}\".format(step, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
