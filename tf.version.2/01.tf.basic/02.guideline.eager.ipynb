{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guideline on Eager Execution\n",
    "\n",
    "* 이 코드는 [TensorFlow official Guide `eager execution` 문서](https://www.tensorflow.org/guide/eager)를 정리한 것이다.\n",
    "\n",
    "[Eager execution](https://www.tensorflow.org/guide/eager#build_a_model) is a flexible machine learning platform for research and experimentation, providing:\n",
    "\n",
    "* **An intuitive interface**: Structure your code naturally and use Python data structures. Quickly iterate on small models and small data.\n",
    "* **Easier debugging**: Call ops directly to inspect running models and test changes. Use standard Python debugging tools for immediate error reporting.\n",
    "* **Natural control flow**: Use Python control flow instead of graph control flow, simplifying the specification of dynamic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(tf.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guideline on building a model\n",
    "\n",
    "* Use only [`tf.keras.layers`](https://www.tensorflow.org/api_docs/python/tf/keras/layers), do **NOT** use `tf.layers` or `tf.contrib.layers`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of a layer made myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySimpleLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, output_units):\n",
    "    super(MySimpleLayer, self).__init__()\n",
    "    self.output_units = output_units\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    # The build method gets called the first time your layer is used.\n",
    "    # Creating variables on build() allows you to make their shape depend\n",
    "    # on the input shape and hence removes the need for the user to specify\n",
    "    # full shapes. It is possible to create variables during __init__() if\n",
    "    # you already know their full shapes.\n",
    "    self.kernel = self.add_variable(\n",
    "      \"kernel\", [input_shape[-1], self.output_units])\n",
    "\n",
    "  def call(self, input):\n",
    "    # Override call() instead of __call__ so we can perform some bookkeeping.\n",
    "    return tf.matmul(input, self.kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple method of building a model\n",
    "* Use [`tf.keras.Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential)\n",
    "* Must set input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  layers.Dense(10, input_shape=(784,)),  # must declare input shape\n",
    "  layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another method of builing a model\n",
    "\n",
    "* Organize models in classes by inheriting from [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model).\n",
    "* It's not required to set an input shape for the [`tf.keras.Model`](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model) class since the parameters are set the first time input is passed to the layer.\n",
    "* [`tf.keras.layers`](https://www.tensorflow.org/api_docs/python/tf/keras/layers) classes create and contain their own model variables that are tied to the lifetime of their layer objects. To share layer variables, share their objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(MNISTModel, self).__init__()\n",
    "    self.dense1 = layers.Dense(units=10)\n",
    "    self.dense2 = layers.Dense(units=10)\n",
    "\n",
    "  def call(self, input):\n",
    "    \"\"\"Run the model.\"\"\"\n",
    "    result = self.dense1(input)\n",
    "    result = self.dense2(result)\n",
    "    result = self.dense2(result)  # reuse variables from dense2 layer\n",
    "    return result\n",
    "\n",
    "model = MNISTModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing gradients\n",
    "* Use [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable([[1.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "  loss = w * w\n",
    "\n",
    "grad = tape.gradient(loss, w)\n",
    "print(grad)  # => tf.Tensor([[ 2.]], shape=(1, 1), dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and format the mnist data\n",
    "(mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "  (tf.cast(mnist_images[..., tf.newaxis]/255., tf.float32), # [..., tf.newaxis] for just expand_dims(last_axis)\n",
    "   tf.cast(mnist_labels,tf.int64)))\n",
    "dataset = dataset.shuffle(1000).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "mnist_model = tf.keras.Sequential([\n",
    "  layers.Conv2D(16, [3,3], activation='relu'),\n",
    "  layers.Conv2D(16, [3,3], activation='relu'),\n",
    "  layers.GlobalAveragePooling2D(),\n",
    "  layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without training, just inference a model in eager execution:\n",
    "for images,labels in dataset.take(1):\n",
    "  print(\"Logits: \", mnist_model(images[0:1]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (batch, (images, labels)) in enumerate(dataset.take(400)): # just 400 steps (iterations), NOT epochs\n",
    "  if batch % 80 == 0:\n",
    "    print()\n",
    "  print('.', end='')\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = mnist_model(images, training=True)\n",
    "    loss_value = tf.losses.sparse_softmax_cross_entropy(labels, logits)\n",
    "\n",
    "  loss_history.append(loss_value.numpy())\n",
    "  grads = tape.gradient(loss_value, mnist_model.variables)\n",
    "  optimizer.apply_gradients(zip(grads, mnist_model.variables),\n",
    "                            global_step=tf.train.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel('Batch #')\n",
    "plt.ylabel('Loss [entropy]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model made myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(Model, self).__init__()\n",
    "    self.W = tf.Variable(5., name='weight')\n",
    "    self.B = tf.Variable(10., name='bias')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs * self.W + self.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 2000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The loss function to be optimized\n",
    "def loss(model, inputs, targets):\n",
    "  error = model(inputs) - targets\n",
    "  return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "  with tf.GradientTape() as tape:\n",
    "    loss_value = loss(model, inputs, targets)\n",
    "  return tape.gradient(loss_value, [model.W, model.B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define:\n",
    "# 1. A model.\n",
    "# 2. Derivatives of a loss function with respect to model parameters.\n",
    "# 3. A strategy for updating the variables based on the derivatives.\n",
    "model = Model()\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "# Training loop\n",
    "for i in range(300):\n",
    "  grads = grad(model, training_inputs, training_outputs)\n",
    "  optimizer.apply_gradients(zip(grads, [model.W, model.B]),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "  if i % 20 == 0:\n",
    "    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "print(\"W = {}, B = {}\".format(model.W.numpy(), model.B.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  layers.Conv2D(16, [3,3], activation='relu'),\n",
    "  layers.GlobalAveragePooling2D(),\n",
    "  layers.Dense(10)\n",
    "])\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "checkpoint_dir = './model_dir'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "root = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                           model=model,\n",
    "                           optimizer_step=tf.train.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a model\n",
    "root.save(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore a model\n",
    "root.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
