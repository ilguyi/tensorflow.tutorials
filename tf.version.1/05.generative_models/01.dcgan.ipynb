{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN with MNIST\n",
    "\n",
    "* `Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks`, [arXiv:1511.06434](https://arxiv.org/abs/1511.06434)\n",
    "  * Alec Radford, Luke Metz and Soumith Chintala\n",
    "  \n",
    "* **DCGAN** using [`tf.contrib.slim`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim)\n",
    "* Use transposed_conv2d and conv2d for Generator and Discriminator, respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "#np.random.seed(219)\n",
    "\n",
    "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Flags (hyperparameter configuration)\n",
    "train_dir = 'train/01.dcgan/exp1/'\n",
    "max_epochs = 30\n",
    "save_epochs = 10\n",
    "summary_steps = 2500\n",
    "print_steps = 1\n",
    "batch_size = 64\n",
    "learning_rate_D = 0.0002\n",
    "learning_rate_G = 0.001\n",
    "k = 1 # the number of step of learning D before learning G\n",
    "num_samples = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), _ = \\\n",
    "    tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_data = train_data\n",
    "train_data = train_data / 255.\n",
    "train_labels = np.asarray(train_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up dataset with `tf.data`\n",
    "\n",
    "### create input pipeline with `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(219)\n",
    "\n",
    "# for train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_data)\n",
    "train_dataset = train_dataset.shuffle(buffer_size = 10000)\n",
    "train_dataset = train_dataset.repeat(count=max_epochs)\n",
    "train_dataset = train_dataset.batch(batch_size = batch_size)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "  \"\"\"Deep Convolutional Generative Adversarial Networks\n",
    "  implementation based on http://arxiv.org/abs/1511.06434\n",
    "  \n",
    "  \"Unsupervised Representation Learning with\n",
    "  Deep Convolutional Generative Adversarial Networks\"\n",
    "  Alec Radford, Luke Metz and Soumith Chintala\n",
    "  \"\"\"\n",
    "  \n",
    "  def __init__(self, mode, train_dataset=None, test_dataset=None):\n",
    "    \"\"\"Basic setup.\n",
    "    \n",
    "    Args:\n",
    "      mode (`string`): 'train' or 'generate'.\n",
    "      train_dataset (`tf.data.Dataset`): train_dataset.\n",
    "      test_dataset (`tf.data.Dataset`): test_dataset.\n",
    "    \"\"\"\n",
    "    assert mode in ['train', 'generate']\n",
    "    self.mode = mode\n",
    "    \n",
    "    # hyper-parameters for model\n",
    "    self.x_dim = 28\n",
    "    self.z_dim = 100\n",
    "    self.batch_size = batch_size\n",
    "    self.num_samples = num_samples\n",
    "    self.train_dataset = train_dataset\n",
    "    self.test_dataset = test_dataset\n",
    "    \n",
    "    # Global step Tensor.\n",
    "    self.global_step = None\n",
    "    \n",
    "    print('The mode is %s.' % self.mode)\n",
    "    print('complete initializing model.')\n",
    "    \n",
    "    \n",
    "  def build_random_z_inputs(self):\n",
    "    \"\"\"Build a vector random_z in latent space.\n",
    "    \n",
    "    Returns:\n",
    "      self.random_z (`2-rank Tensor` with [batch_size, z_dim]):\n",
    "          latent vector which size is generally 100 dim.\n",
    "      self.sample_random_z (`2-rank Tensor` with [num_samples, z_dim]):\n",
    "          latent vector which size is generally 100 dim.\n",
    "    \"\"\"\n",
    "    # Setup variable of random vector z\n",
    "    with tf.variable_scope('random_z'):\n",
    "      if self.mode == 'generate':\n",
    "        self.random_z = tf.placeholder(dtype=tf.float32, shape=[None, 1, 1, self.z_dim])\n",
    "        return self.random_z\n",
    "      \n",
    "      self.random_z = tf.random_uniform([self.batch_size, 1, 1, self.z_dim],\n",
    "                                        minval=-1.0, maxval=1.0)\n",
    "      self.sample_random_z = tf.random_uniform([self.num_samples, 1, 1, self.z_dim],\n",
    "                                               minval=-1.0, maxval=1.0)\n",
    "      return self.random_z, self.sample_random_z\n",
    "  \n",
    "  \n",
    "  def read_MNIST(self, dataset):\n",
    "    \"\"\"Read MNIST dataset\n",
    "    \n",
    "    Args:\n",
    "      dataset (`tf.data.Dataset` format): MNIST dataset.\n",
    "      \n",
    "    Returns:\n",
    "      self.mnist (`4-rank Tensor` with [batch, x_dim, x_dim, 1]): MNIST dataset with batch size.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('mnist'):\n",
    "      iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "      self.mnist = iterator.get_next()\n",
    "      self.mnist = tf.cast(self.mnist, dtype = tf.float32)\n",
    "      self.mnist = tf.expand_dims(self.mnist, axis=3)\n",
    "      \n",
    "    return self.mnist\n",
    "\n",
    "\n",
    "  def Generator(self, random_z, is_training=True, reuse=False):\n",
    "    \"\"\"Generator setup.\n",
    "    \n",
    "    Args:\n",
    "      random_z (`2-rank Tensor` with [batch_size, z_dim]):\n",
    "          latent vector which size is generally 100 dim.\n",
    "      is_training (`bool`): whether training mode or test mode.\n",
    "      reuse (`bool`): whether variable reuse or not.\n",
    "      \n",
    "    Returns:\n",
    "      generated_data (`4-rank Tensor` with [batch_size, h, w, c])\n",
    "          generated images from random vector z.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Generator', reuse=reuse) as scope:\n",
    "      batch_norm_params = {'decay': 0.9,\n",
    "                           'epsilon': 0.001,\n",
    "                           'is_training': is_training,\n",
    "                           'scope': 'batch_norm'}\n",
    "      with slim.arg_scope([slim.conv2d_transpose],\n",
    "                          kernel_size=[4, 4],\n",
    "                          stride=[2, 2],\n",
    "                          normalizer_fn=slim.batch_norm,\n",
    "                          normalizer_params=batch_norm_params):\n",
    "\n",
    "        # Use full conv2d_transpose instead of projection and reshape\n",
    "        # random_z: 100 dim\n",
    "        self.inputs = random_z\n",
    "        # inputs = 1 x 1 x 100 dim\n",
    "        self.layer1 = slim.conv2d_transpose(inputs=self.inputs,\n",
    "                                            num_outputs=256,\n",
    "                                            kernel_size=[3, 3],\n",
    "                                            padding='VALID',\n",
    "                                            scope='layer1')\n",
    "        # layer1: 3 x 3 x 256 dim\n",
    "        self.layer2 = slim.conv2d_transpose(inputs=self.layer1,\n",
    "                                            num_outputs=128,\n",
    "                                            kernel_size=[3, 3],\n",
    "                                            padding='VALID',\n",
    "                                            scope='layer2')\n",
    "        # layer2: 7 x 7 x 128 dim\n",
    "        self.layer3 = slim.conv2d_transpose(inputs=self.layer2,\n",
    "                                            num_outputs=64,\n",
    "                                            scope='layer3')\n",
    "        # layer3: 14 x 14 x 64 dim\n",
    "        self.layer4 = slim.conv2d_transpose(inputs=self.layer3,\n",
    "                                            num_outputs=1,\n",
    "                                            normalizer_fn=None,\n",
    "                                            activation_fn=tf.sigmoid,\n",
    "                                            scope='layer4')\n",
    "        # output = layer4: 28 x 28 x 1 dim\n",
    "        generated_data = self.layer4\n",
    "\n",
    "        return generated_data\n",
    "    \n",
    "    \n",
    "  def Discriminator(self, data, reuse=False):\n",
    "    \"\"\"Discriminator setup.\n",
    "    \n",
    "    Args:\n",
    "      data (`2-rank Tensor` with [batch_size, x_dim]): MNIST real data.\n",
    "      reuse (`bool`): whether variable reuse or not.\n",
    "      \n",
    "    Returns:\n",
    "      logits (`1-rank Tensor` with [batch_size]): logits of data.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('Discriminator', reuse=reuse) as scope:\n",
    "      batch_norm_params = {'decay': 0.9,\n",
    "                           'epsilon': 0.001,\n",
    "                           'scope': 'batch_norm'}\n",
    "      with slim.arg_scope([slim.conv2d],\n",
    "                          kernel_size=[4, 4],\n",
    "                          stride=[2, 2],\n",
    "                          activation_fn=tf.nn.leaky_relu,\n",
    "                          normalizer_fn=slim.batch_norm,\n",
    "                          normalizer_params=batch_norm_params):\n",
    "\n",
    "        # data: 28 x 28 x 1 dim\n",
    "        self.layer1 = slim.conv2d(inputs=data,\n",
    "                                  num_outputs=64,\n",
    "                                  normalizer_fn=None,\n",
    "                                  scope='layer1')\n",
    "        # layer1: 14 x 14 x 64 dim\n",
    "        self.layer2 = slim.conv2d(inputs=self.layer1,\n",
    "                                  num_outputs=128,\n",
    "                                  scope='layer2')\n",
    "        # layer2: 7 x 7 x 128 dim\n",
    "        self.layer3 = slim.conv2d(inputs=self.layer2,\n",
    "                                  num_outputs=256,\n",
    "                                  kernel_size=[3, 3],\n",
    "                                  padding='VALID',\n",
    "                                  scope='layer3')\n",
    "        # layer3: 3 x 3 x 256 dim\n",
    "        self.layer4 = slim.conv2d(inputs=self.layer3,\n",
    "                                  num_outputs=1,\n",
    "                                  kernel_size=[3, 3],\n",
    "                                  stride=[1, 1],\n",
    "                                  padding='VALID',\n",
    "                                  normalizer_fn=None,\n",
    "                                  activation_fn=None,\n",
    "                                  scope='layer4')\n",
    "        # logits = layer4: 1 x 1 x 1 dim\n",
    "        discriminator_logits = tf.squeeze(self.layer4, axis=[1, 2])\n",
    "\n",
    "        return discriminator_logits\n",
    "    \n",
    "    \n",
    "  def setup_global_step(self):\n",
    "    \"\"\"Sets up the global step Tensor.\"\"\"\n",
    "    if self.mode == \"train\":\n",
    "      self.global_step = tf.train.get_or_create_global_step()\n",
    "      \n",
    "      print('complete setup global_step.')\n",
    "      \n",
    "      \n",
    "  def GANLoss(self, logits, is_real=True, scope=None):\n",
    "    \"\"\"Computes standard GAN loss between `logits` and `labels`.\n",
    "    \n",
    "    Args:\n",
    "      logits (`1-rank Tensor`): logits.\n",
    "      is_real (`bool`): True means `1` labeling, False means `0` labeling.\n",
    "      \n",
    "    Returns:\n",
    "      loss (`0-randk Tensor): the standard GAN loss value. (binary_cross_entropy)\n",
    "    \"\"\"\n",
    "    if is_real:\n",
    "      labels = tf.ones_like(logits)\n",
    "    else:\n",
    "      labels = tf.zeros_like(logits)\n",
    "\n",
    "    loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=labels,\n",
    "                                           logits=logits,\n",
    "                                           scope=scope)\n",
    "\n",
    "    return loss\n",
    "\n",
    "      \n",
    "  def build(self):\n",
    "    \"\"\"Creates all ops for training or generate.\"\"\"\n",
    "    self.setup_global_step()\n",
    "    \n",
    "    if self.mode == \"generate\":\n",
    "      # generating random vector\n",
    "      self.random_z = self.build_random_z_inputs()\n",
    "      # generating images from Generator() via random vector z\n",
    "      self.generated_data = self.Generator(self.random_z, is_training=False)\n",
    "    \n",
    "    else:\n",
    "      # generating random vector\n",
    "      self.random_z, self.sample_random_z = self.build_random_z_inputs()\n",
    "      # generating images from Generator() via random vector z\n",
    "      self.generated_data = self.Generator(self.random_z)\n",
    "      \n",
    "      # read dataset\n",
    "      self.real_data = self.read_MNIST(self.train_dataset)\n",
    "      \n",
    "      # discriminating real data by Discriminator()\n",
    "      self.real_logits = self.Discriminator(self.real_data)\n",
    "      # discriminating fake data (generated)_images) by Discriminator()\n",
    "      self.fake_logits = self.Discriminator(self.generated_data, reuse=True)\n",
    "      \n",
    "      # losses of real with label \"1\"\n",
    "      self.loss_real = self.GANLoss(logits=self.real_logits, is_real=True, scope='loss_D_real')\n",
    "      # losses of fake with label \"0\"\n",
    "      self.loss_fake = self.GANLoss(logits=self.fake_logits, is_real=False, scope='loss_D_fake')\n",
    "      \n",
    "      # losses of Discriminator\n",
    "      with tf.variable_scope('loss_D'):\n",
    "        self.loss_Discriminator = self.loss_real + self.loss_fake\n",
    "      # losses of Generator with label \"1\" that used to fool the Discriminator\n",
    "      self.loss_Generator = self.GANLoss(logits=self.fake_logits, is_real=True, scope='loss_G')\n",
    "      \n",
    "      # Separate variables for each function\n",
    "      self.D_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Discriminator')\n",
    "      self.G_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Generator')\n",
    "      \n",
    "      \n",
    "      # generating images for sample\n",
    "      self.sample_data = self.Generator(self.sample_random_z, is_training=False, reuse=True)\n",
    "      \n",
    "      # write summaries\n",
    "      # Add loss summaries\n",
    "      tf.summary.scalar('losses/loss_Discriminator', self.loss_Discriminator)\n",
    "      tf.summary.scalar('losses/loss_Generator', self.loss_Generator)\n",
    "      \n",
    "      # Add histogram summaries\n",
    "      for var in self.D_vars:\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "      for var in self.G_vars:\n",
    "        tf.summary.histogram(var.op.name, var)\n",
    "      \n",
    "      # Add image summaries\n",
    "      tf.summary.image('random_images', self.generated_data, max_outputs=4)\n",
    "      tf.summary.image('real_images', self.real_data)\n",
    "      \n",
    "    print('complete model build.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define plot function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sample_data(sample_data, max_print=num_samples):\n",
    "  print_images = sample_data[:max_print,:]\n",
    "  print_images = print_images.reshape([max_print, 28, 28])\n",
    "  print_images = print_images.swapaxes(0, 1)\n",
    "  print_images = print_images.reshape([28, max_print * 28])\n",
    "  \n",
    "  plt.figure(figsize=(max_print, 1))\n",
    "  plt.axis('off')\n",
    "  plt.imshow(print_images, cmap='gray')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DCGAN(mode=\"train\", train_dataset=train_dataset)\n",
    "model.build()\n",
    "\n",
    "# show info for trainable variables\n",
    "t_vars = tf.trainable_variables()\n",
    "slim.model_analyzer.analyze_vars(t_vars, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_D = tf.train.AdamOptimizer(learning_rate=learning_rate_D, beta1=0.5)\n",
    "opt_G = tf.train.AdamOptimizer(learning_rate=learning_rate_G, beta1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='Discriminator')):\n",
    "  opt_D_op = opt_D.minimize(model.loss_Discriminator, var_list=model.D_vars)\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope='Generator')):\n",
    "  opt_G_op = opt_G.minimize(model.loss_Generator, global_step=model.global_step,\n",
    "                            var_list=model.G_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign `tf.summary.FileWriter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_location = train_dir\n",
    "print('Saving graph to: %s' % graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(tf.get_default_graph()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.train.Saver`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `tf.Session` and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_ = time.time()\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  tf.logging.info('Start Session.')\n",
    "  \n",
    "  num_examples = len(train_data)\n",
    "  num_batches_per_epoch = int(num_examples / batch_size)\n",
    "  \n",
    "  # save loss values for plot\n",
    "  loss_history = []\n",
    "  pre_epochs = 0\n",
    "  while True:\n",
    "    try:\n",
    "      start_time = time.time()\n",
    "      \n",
    "      for _ in range(k):\n",
    "        _, loss_D = sess.run([opt_D_op, model.loss_Discriminator])\n",
    "      _, global_step_, loss_G = sess.run([opt_G_op,\n",
    "                                          model.global_step,\n",
    "                                          model.loss_Generator])\n",
    "      \n",
    "      epochs = global_step_ * batch_size / float(num_examples)\n",
    "      duration = time.time() - start_time\n",
    "\n",
    "      if global_step_ % print_steps == 0:\n",
    "        clear_output(wait=True)\n",
    "        examples_per_sec = batch_size / float(duration)\n",
    "        print(\"Epochs: {:.2f} global_step: {} loss_D: {:.3f} loss_G: {:.3f} ({:.2f} examples/sec; {:.3f} sec/batch)\".format(\n",
    "                  epochs, global_step_, loss_D, loss_G, examples_per_sec, duration))\n",
    "\n",
    "        loss_history.append([epochs, loss_D, loss_G])\n",
    "\n",
    "        # print sample data\n",
    "        sample_data = sess.run(model.sample_data)\n",
    "        print_sample_data(sample_data)\n",
    "\n",
    "      # write summaries periodically\n",
    "      if global_step_ % summary_steps == 0:\n",
    "        summary_str = sess.run(summary_op)\n",
    "        train_writer.add_summary(summary_str, global_step=global_step_)\n",
    "\n",
    "      # save model checkpoint periodically\n",
    "      if int(epochs) % save_epochs == 0  and  pre_epochs != int(epochs):\n",
    "        tf.logging.info('Saving model with global step {} (= {} epochs) to disk.'.format(global_step_, int(epochs)))\n",
    "        saver.save(sess, train_dir + 'model.ckpt', global_step=global_step_)\n",
    "        pre_epochs = int(epochs)\n",
    "        \n",
    "    except tf.errors.OutOfRangeError:\n",
    "      print(\"End of dataset\")  # ==> \"End of dataset\"\n",
    "      tf.logging.info('Saving model with global step {} (= {} epochs) to disk.'.format(global_step_, int(epochs)))\n",
    "      saver.save(sess, train_dir + 'model.ckpt', global_step=global_step_)\n",
    "      break\n",
    "      \n",
    "  tf.logging.info('complete training...')\n",
    "  tf.logging.info('total duration: {}'.format(time.time() - start_time_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = np.asarray(loss_history)\n",
    "\n",
    "plt.plot(loss_history[:,0], loss_history[:,1], label='loss_D')\n",
    "plt.plot(loss_history[:,0], loss_history[:,2], label='loss_G')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Generator function from latest checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = train_dir\n",
    "checkpoint_step = None # None means the latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not tf.gfile.IsDirectory(checkpoint_dir):\n",
    "  raise ValueError('checkpoint_dir must be folder path')\n",
    "\n",
    "model = DCGAN(mode=\"generate\")\n",
    "model.build()\n",
    "\n",
    "# show info for global variables\n",
    "g_vars = tf.global_variables()\n",
    "slim.model_analyzer.analyze_vars(g_vars, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "if checkpoint_step is not None:\n",
    "  checkpoint_path = os.path.join(checkpoint_dir, 'model.ckpt-%d' % checkpoint_step)\n",
    "else:\n",
    "  checkpoint_path = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "  tf.logging.info(\"Loading model from checkpoint: %s\", checkpoint_path)\n",
    "  saver.restore(sess, checkpoint_path)\n",
    "  tf.logging.info(\"Successfully loaded checkpoint: %s\", os.path.basename(checkpoint_path))\n",
    "  \n",
    "  # if you want the fixed random number\n",
    "  #np.random.seed(219)\n",
    "  random_z = np.random.uniform(-1, 1, [num_samples, 1, 1, model.z_dim])\n",
    "  generated_image = sess.run(model.generated_data,\n",
    "                             feed_dict={model.random_z: random_z})\n",
    "  \n",
    "  tf.logging.info('complete generating...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_sample_data(generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sample_data(sample_data, checkpoint_step, max_print=num_samples):\n",
    "  \"\"\"save image\n",
    "  \n",
    "  Args:\n",
    "    sample_data (np.float32): 2-rank np.array [batch, 784]\n",
    "    max_print (int): number of images for saving\n",
    "    checkpoint_step (int): checkpoint_step you want to generate images\n",
    "  \"\"\"  \n",
    "  images = sample_data[:max_print,:]\n",
    "  images = images.reshape([max_print, 28, 28])\n",
    "  images = images.swapaxes(0, 1)\n",
    "  images = images.reshape([28, max_print * 28])  \n",
    "  save_image = Image.fromarray(np.uint8(images * 255))\n",
    "  \n",
    "  if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "  filename = 'results/dcgan.result.ckpt.'\n",
    "  if checkpoint_step is not None:\n",
    "    filename += '%d' % int(checkpoint_step) + '.jpg'\n",
    "  else:\n",
    "    latest_checkpoint_step = tf.train.latest_checkpoint(checkpoint_dir).split('-')[-1]\n",
    "    filename += '%d' % int(latest_checkpoint_step) + '.jpg'\n",
    "  \n",
    "  save_image.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_sample_data(generated_image, checkpoint_step, num_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
